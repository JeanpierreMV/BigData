{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTyBjWTHEiSt",
        "outputId": "2c533157-d607-427f-aee3-7f8981f9fd8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=4f90233fe38c45d0ed13fc8697cca260b37cc942538a78c28bc7c694f623e76b\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install geopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3yg4ZMXBZU6",
        "outputId": "7950bd6d-30ff-441b-caa4-635789e28cc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.10/dist-packages (2.3.0)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.10/dist-packages (from geopy) (2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
        "from datetime import datetime\n",
        "from geopy.geocoders import Nominatim\n",
        "from pyspark.sql.functions import lit\n",
        "\n",
        "# Lista de lugares con sus coordenadas\n",
        "lugares = [\n",
        "    {\"nombre\": \"Amazonas/Peru\", \"lat\": -4.999999999682269, \"lon\": -78},\n",
        "    {\"nombre\": \"Ancash/Peru\", \"lat\": -9.499999999871093, \"lon\": -77.75},\n",
        "    # Agrega más lugares si es necesario\n",
        "]\n",
        "\n",
        "# Crear una sesión Spark en Colab\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"WeatherAnalysis\").getOrCreate()\n",
        "\n",
        "# Definir el esquema para el DataFrame de Spark\n",
        "schema = StructType([\n",
        "    # Define tus estructuras aquí según tus necesidades\n",
        "])\n",
        "\n",
        "# Crear una lista para almacenar los DataFrames individuales\n",
        "dataframes = []\n",
        "\n",
        "# Iterar sobre la lista de lugares\n",
        "for lugar in lugares:\n",
        "    # Obtener los datos de la API\n",
        "    url = f\"https://api.openweathermap.org/data/3.0/onecall?lat={lugar['lat']}&lon={lugar['lon']}&exclude=hourly,minutely&appid=107298a9f77f36886d2b680a82c145cf\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "\n",
        "    # Extraer la información diaria de la respuesta\n",
        "    daily_data = data.get(\"daily\", [])\n",
        "\n",
        "    # Convertir datos a un DataFrame de pandas\n",
        "    df = pd.json_normalize(daily_data)\n",
        "\n",
        "    # Convertir la columna \"dt\" a formato datetime\n",
        "    df[\"dt\"] = pd.to_datetime(df[\"dt\"], unit=\"s\")\n",
        "\n",
        "    # Convertir el DataFrame de pandas en un DataFrame de Spark\n",
        "    spark_df = spark.createDataFrame(df)\n",
        "\n",
        "    # Obtener la información de la ubicación (región y país)\n",
        "    geolocator = Nominatim(user_agent=\"weather_analysis\")\n",
        "    location = geolocator.reverse((lugar[\"lat\"], lugar[\"lon\"]), language=\"en\")\n",
        "\n",
        "    # Extraer información de ubicación\n",
        "    region = location.raw['address']['state']\n",
        "    country = location.raw['address']['country']\n",
        "\n",
        "    # Agregar las columnas de ubicación al DataFrame de Spark\n",
        "    spark_df = spark_df.withColumn(\"region\", lit(region))\n",
        "    spark_df = spark_df.withColumn(\"country\", lit(country))\n",
        "\n",
        "    # Agregar una columna con el nombre del lugar\n",
        "    spark_df = spark_df.withColumn(\"nombre_lugar\", lit(lugar[\"nombre\"]))\n",
        "\n",
        "    # Agregar el DataFrame actual a la lista\n",
        "    dataframes.append(spark_df)\n",
        "\n",
        "# Unir todos los DataFrames en uno solo\n",
        "consolidated_df = dataframes[0]\n",
        "for df in dataframes[1:]:\n",
        "    consolidated_df = consolidated_df.union(df)\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame consolidado\n",
        "print(\"Información consolidada:\")\n",
        "consolidated_df.printSchema()\n",
        "consolidated_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B1boxqsEzi5",
        "outputId": "39949eb4-f466-417e-c24d-f06be5c22a6f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Información consolidada:\n",
            "root\n",
            " |-- dt: timestamp (nullable = true)\n",
            " |-- sunrise: long (nullable = true)\n",
            " |-- sunset: long (nullable = true)\n",
            " |-- moonrise: long (nullable = true)\n",
            " |-- moonset: long (nullable = true)\n",
            " |-- moon_phase: double (nullable = true)\n",
            " |-- summary: string (nullable = true)\n",
            " |-- pressure: long (nullable = true)\n",
            " |-- humidity: long (nullable = true)\n",
            " |-- dew_point: double (nullable = true)\n",
            " |-- wind_speed: double (nullable = true)\n",
            " |-- wind_deg: long (nullable = true)\n",
            " |-- wind_gust: double (nullable = true)\n",
            " |-- weather: array (nullable = true)\n",
            " |    |-- element: map (containsNull = true)\n",
            " |    |    |-- key: string\n",
            " |    |    |-- value: long (valueContainsNull = true)\n",
            " |-- clouds: long (nullable = true)\n",
            " |-- pop: double (nullable = true)\n",
            " |-- rain: double (nullable = true)\n",
            " |-- uvi: double (nullable = true)\n",
            " |-- temp.day: double (nullable = true)\n",
            " |-- temp.min: double (nullable = true)\n",
            " |-- temp.max: double (nullable = true)\n",
            " |-- temp.night: double (nullable = true)\n",
            " |-- temp.eve: double (nullable = true)\n",
            " |-- temp.morn: double (nullable = true)\n",
            " |-- feels_like.day: double (nullable = true)\n",
            " |-- feels_like.night: double (nullable = true)\n",
            " |-- feels_like.eve: double (nullable = true)\n",
            " |-- feels_like.morn: double (nullable = true)\n",
            " |-- region: string (nullable = false)\n",
            " |-- country: string (nullable = false)\n",
            " |-- nombre_lugar: string (nullable = false)\n",
            "\n",
            "+-------------------+----------+----------+----------+----------+----------+--------------------+--------+--------+---------+----------+--------+---------+--------------------+------+----+-----+-----+--------+--------+--------+----------+--------+---------+--------------+----------------+--------------+---------------+--------+-------+-------------+\n",
            "|                 dt|   sunrise|    sunset|  moonrise|   moonset|moon_phase|             summary|pressure|humidity|dew_point|wind_speed|wind_deg|wind_gust|             weather|clouds| pop| rain|  uvi|temp.day|temp.min|temp.max|temp.night|temp.eve|temp.morn|feels_like.day|feels_like.night|feels_like.eve|feels_like.morn|  region|country| nombre_lugar|\n",
            "+-------------------+----------+----------+----------+----------+----------+--------------------+--------+--------+---------+----------+--------+---------+--------------------+------+----+-----+-----+--------+--------+--------+----------+--------+---------+--------------+----------------+--------------+---------------+--------+-------+-------------+\n",
            "|2023-11-25 16:00:00|1700909278|1700953817|1700948820|1700903280|      0.44|Expect a day of p...|    1013|      61|   293.59|      1.97|      76|     2.81|[{icon -> NULL, d...|    97|0.96| 6.21| 9.23|  303.06|  294.72|  306.31|    294.72|  302.39|   294.92|        306.01|          295.52|        305.46|         295.71|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-26 16:00:00|1700995692|1701040240|1701038580|1700992680|      0.48|Expect a day of p...|    1014|      99|   294.04|       1.4|     186|      3.4|[{icon -> NULL, d...|   100| 1.0|93.66|10.62|   295.3|  294.56|  296.93|    294.56|   296.6|   295.02|        296.16|          295.37|        297.51|         295.87|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-27 16:00:00|1701082106|1701126664|1701128340|1701082260|       0.5|Expect a day of p...|    1015|      95|   295.38|      1.27|     176|     2.21|[{icon -> NULL, d...|    97|0.96|10.77|10.57|  297.46|  294.21|  302.23|    294.94|  302.23|   294.25|        298.43|          295.58|        305.15|          295.0|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-28 17:00:00|1701168522|1701213089|1701218280|1701171960|      0.55|You can expect pa...|    1010|      60|   294.07|      2.29|      73|      2.7|[{icon -> NULL, d...|    67| 1.0|15.32|12.02|  303.99|  294.06|  303.99|    295.22|  295.84|   296.18|        307.66|          296.09|        296.72|         296.73|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-29 17:00:00|1701254938|1701299514|1701307980|1701261720|      0.58|Expect a day of p...|    1011|      89|   295.42|      1.11|     176|     1.31|[{icon -> NULL, d...|   100|0.84| 4.69| 0.06|  298.47|  294.74|  302.98|    295.09|  296.76|   295.55|        299.38|          295.72|         297.4|         296.43|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-30 17:00:00|1701341355|1701385940|1701397620|1701351480|      0.61|Expect a day of p...|    1008|      55|   293.17|      1.91|      73|      2.7|[{icon -> NULL, d...|    48| 1.0|12.27|  1.0|  304.61|  293.42|  304.61|    294.93|  295.73|    295.0|         307.7|          295.75|         296.6|          295.7|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-12-01 17:00:00|1701427773|1701472366|1701486960|1701441120|      0.64|You can expect ra...|    1013|      97|   295.02|      0.87|     188|     0.82|[{icon -> NULL, d...|    99| 1.0|15.38|  1.0|  296.67|   294.6|  297.51|     294.6|  295.02|   295.02|        297.61|          295.36|        295.85|         295.85|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-12-02 17:00:00|1701514191|1701558792|1701576060|1701530460|      0.67|Expect a day of p...|    1008|      42|   291.24|       2.1|      36|     3.11|[{icon -> NULL, d...|    66| 0.8| 0.83|  1.0|  306.86|  294.36|  306.86|    296.88|  299.23|    295.3|        308.59|           297.5|        299.23|         296.08|Amazonas|   Peru|Amazonas/Peru|\n",
            "|2023-11-25 16:00:00|1700908799|1700954175|1700949120|1700902920|      0.44|Expect a day of p...|    1016|      63|   279.73|      4.52|     248|     2.81|[{icon -> NULL, d...|    78|0.64| 6.83| 13.9|   287.5|  281.99|   287.5|     283.4|  283.26|   282.11|        286.64|          282.57|        282.68|         280.85|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-11-26 16:00:00|1700995209|1701040603|1701039000|1700992200|      0.48|Expect a day of p...|    1017|      79|   280.47|      3.67|     252|      2.4|[{icon -> NULL, d...|   100|0.72|12.96| 9.54|  284.91|  282.54|  285.39|     283.1|  282.54|   282.78|         284.2|           283.1|        281.82|         282.78|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-11-27 16:00:00|1701081619|1701127031|1701128880|1701081660|       0.5|Expect a day of p...|    1017|      69|   279.87|      3.94|     249|      2.7|[{icon -> NULL, d...|    94|0.76|23.42| 7.58|  286.32|  281.76|  286.32|    282.73|  282.82|   282.19|        285.49|          282.73|        281.28|         281.57|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-11-28 16:00:00|1701168031|1701213460|1701218820|1701171300|      0.55|Expect a day of p...|    1018|      69|   280.29|      4.33|     248|     2.81|[{icon -> NULL, d...|    51|0.68|11.77|14.13|   286.6|  282.15|   286.6|    282.88|  283.63|   282.59|         285.8|          281.76|        283.19|         282.59|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-11-29 16:00:00|1701254443|1701299889|1701308580|1701261060|      0.58|Expect a day of p...|    1015|      61|    278.9|      4.22|     252|     2.52|[{icon -> NULL, d...|    97|0.64| 4.13| 0.59|  287.09|  281.97|  287.09|     283.4|  284.07|   281.97|        286.13|          282.62|        283.54|         281.97|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-11-30 16:00:00|1701340857|1701386318|1701398160|1701350820|      0.61|There will be par...|    1016|      68|   279.79|      3.87|     244|      2.4|[{icon -> NULL, d...|    77|0.68| 8.52|  1.0|  286.42|  282.25|  286.42|    282.37|  283.07|   282.25|        285.58|          281.91|        281.35|         282.25|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-12-01 17:00:00|1701427271|1701472748|1701487380|1701440520|      0.64|There will be rai...|    1017|      92|   281.68|      4.04|     240|     3.03|[{icon -> NULL, d...|   100|0.72|17.07|  1.0|  283.82|  281.88|  284.59|    282.74|  282.21|   282.32|        283.34|          282.74|        282.21|         282.32|  Ancash|   Peru|  Ancash/Peru|\n",
            "|2023-12-02 17:00:00|1701513686|1701559178|1701576420|1701529980|      0.67|Expect a day of p...|    1017|      87|   280.93|      3.99|     250|     2.81|[{icon -> NULL, d...|    76|0.68| 4.13|  1.0|  283.91|  281.94|  286.03|    282.01|  282.24|   283.22|        283.31|          282.01|        282.24|         282.16|  Ancash|   Peru|  Ancash/Peru|\n",
            "+-------------------+----------+----------+----------+----------+----------+--------------------+--------+--------+---------+----------+--------+---------+--------------------+------+----+-----+-----+--------+--------+--------+----------+--------+---------+--------------+----------------+--------------+---------------+--------+-------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
        "from pyspark.sql.functions import monotonically_increasing_id, col, from_unixtime\n",
        "\n",
        "# Crear una sesión Spark en Colab\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"WeatherAnalysis\").getOrCreate()\n",
        "\n",
        "# Definir el esquema para la DimTiempo\n",
        "schema_dim_tiempo = StructType([\n",
        "    StructField(\"idTiempo\", StringType(), False),  # Clave primaria\n",
        "    StructField(\"dt\", TimestampType(), True),\n",
        "    StructField(\"sunrise\", TimestampType(), True),\n",
        "    StructField(\"sunset\", TimestampType(), True),\n",
        "    StructField(\"nombre_lugar\", StringType(), False)  # Nueva columna para el nombre del lugar\n",
        "])\n",
        "\n",
        "# Convertir la columna sunrise de Unix a formato de fecha\n",
        "consolidated_df = consolidated_df.withColumn(\"sunrise\", from_unixtime(\"sunrise\"))\n",
        "\n",
        "# Convertir la columna sunset de Unix a formato de fecha\n",
        "consolidated_df = consolidated_df.withColumn(\"sunset\", from_unixtime(\"sunset\"))\n",
        "\n",
        "# Crear DataFrame para la DimTiempo y ordenar por la columna dt\n",
        "data_dim_tiempo = consolidated_df.select(\"dt\", \"sunrise\", \"sunset\", \"nombre_lugar\").distinct().orderBy(\"dt\")\n",
        "data_dim_tiempo = data_dim_tiempo.withColumn(\"idTiempo\", monotonically_increasing_id())\n",
        "df_dim_tiempo = data_dim_tiempo.select(\"idTiempo\", \"dt\", \"sunrise\", \"sunset\", \"nombre_lugar\")\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame DimTiempo\n",
        "print(\"DimTiempo después de la conversión:\")\n",
        "df_dim_tiempo.printSchema()\n",
        "df_dim_tiempo.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPoZbGFAkXxE",
        "outputId": "b9ae0cc2-3388-493c-fc25-1a2fc636ad91"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimTiempo después de la conversión:\n",
            "root\n",
            " |-- idTiempo: long (nullable = false)\n",
            " |-- dt: timestamp (nullable = true)\n",
            " |-- sunrise: string (nullable = true)\n",
            " |-- sunset: string (nullable = true)\n",
            " |-- nombre_lugar: string (nullable = false)\n",
            "\n",
            "+--------+-------------------+-------------------+-------------------+-------------+\n",
            "|idTiempo|                 dt|            sunrise|             sunset| nombre_lugar|\n",
            "+--------+-------------------+-------------------+-------------------+-------------+\n",
            "|       0|2023-11-25 16:00:00|2023-11-25 10:47:58|2023-11-25 23:10:17|Amazonas/Peru|\n",
            "|       1|2023-11-25 16:00:00|2023-11-25 10:39:59|2023-11-25 23:16:15|  Ancash/Peru|\n",
            "|       2|2023-11-26 16:00:00|2023-11-26 10:48:12|2023-11-26 23:10:40|Amazonas/Peru|\n",
            "|       3|2023-11-26 16:00:00|2023-11-26 10:40:09|2023-11-26 23:16:43|  Ancash/Peru|\n",
            "|       4|2023-11-27 16:00:00|2023-11-27 10:48:26|2023-11-27 23:11:04|Amazonas/Peru|\n",
            "|       5|2023-11-27 16:00:00|2023-11-27 10:40:19|2023-11-27 23:17:11|  Ancash/Peru|\n",
            "|       6|2023-11-28 16:00:00|2023-11-28 10:40:31|2023-11-28 23:17:40|  Ancash/Peru|\n",
            "|       7|2023-11-28 17:00:00|2023-11-28 10:48:42|2023-11-28 23:11:29|Amazonas/Peru|\n",
            "|       8|2023-11-29 16:00:00|2023-11-29 10:40:43|2023-11-29 23:18:09|  Ancash/Peru|\n",
            "|       9|2023-11-29 17:00:00|2023-11-29 10:48:58|2023-11-29 23:11:54|Amazonas/Peru|\n",
            "|      10|2023-11-30 16:00:00|2023-11-30 10:40:57|2023-11-30 23:18:38|  Ancash/Peru|\n",
            "|      11|2023-11-30 17:00:00|2023-11-30 10:49:15|2023-11-30 23:12:20|Amazonas/Peru|\n",
            "|      12|2023-12-01 17:00:00|2023-12-01 10:49:33|2023-12-01 23:12:46|Amazonas/Peru|\n",
            "|      13|2023-12-01 17:00:00|2023-12-01 10:41:11|2023-12-01 23:19:08|  Ancash/Peru|\n",
            "|      14|2023-12-02 17:00:00|2023-12-02 10:49:51|2023-12-02 23:13:12|Amazonas/Peru|\n",
            "|      15|2023-12-02 17:00:00|2023-12-02 10:41:26|2023-12-02 23:19:38|  Ancash/Peru|\n",
            "+--------+-------------------+-------------------+-------------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lugares = [\n",
        "    {\"nombre\": \"Amazonas/Peru\", \"lat\": -4.999999999682269, \"lon\": -78},\n",
        "    {\"nombre\": \"Ancash/Peru\", \"lat\": -9.499999999871093, \"lon\": -77.75},\n",
        "    # Agrega más lugares si es necesario\n",
        "]"
      ],
      "metadata": {
        "id": "XhkfTZcjsB8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, lit, when\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import IntegerType, StringType, StructType, StructField, DoubleType\n",
        "\n",
        "\n",
        "# Definir el esquema para la DimUbicacion\n",
        "schema_dim_ubicacion = StructType([\n",
        "    StructField(\"idUbicacion\", IntegerType(), False),  # Clave primaria\n",
        "    StructField(\"region\", StringType(), True),\n",
        "    StructField(\"country\", StringType(), True),\n",
        "    StructField(\"nombre_lugar\", StringType(), True),\n",
        "    StructField(\"lat\", DoubleType(), True),\n",
        "    StructField(\"lon\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Obtener datos de ubicación únicos\n",
        "data_dim_ubicacion = consolidated_df.select(\"region\", \"country\", \"nombre_lugar\").distinct()\n",
        "\n",
        "# Asignar identificadores únicos a través de row_number y Window\n",
        "window = Window.orderBy(\"region\", \"country\", \"nombre_lugar\")\n",
        "data_dim_ubicacion = data_dim_ubicacion.withColumn(\"idUbicacion\", row_number().over(window))\n",
        "\n",
        "# Crear DataFrame para la DimUbicacion\n",
        "df_dim_ubicacion = data_dim_ubicacion.select(\"idUbicacion\", \"region\", \"country\", \"nombre_lugar\")\n",
        "\n",
        "# Agregar las columnas lat y lon con los datos del array lugares\n",
        "df_dim_ubicacion = df_dim_ubicacion.withColumn(\"lat\", lit(None).cast(DoubleType()))\n",
        "df_dim_ubicacion = df_dim_ubicacion.withColumn(\"lon\", lit(None).cast(DoubleType()))\n",
        "\n",
        "# Actualizar las columnas lat y lon con los valores del array lugares\n",
        "for lugar_info in lugares:\n",
        "    condition = (df_dim_ubicacion[\"nombre_lugar\"] == lugar_info[\"nombre\"])\n",
        "    df_dim_ubicacion = df_dim_ubicacion.withColumn(\"lat\", when(condition, lugar_info[\"lat\"]).otherwise(df_dim_ubicacion[\"lat\"]))\n",
        "    df_dim_ubicacion = df_dim_ubicacion.withColumn(\"lon\", when(condition, lugar_info[\"lon\"]).otherwise(df_dim_ubicacion[\"lon\"]))\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame DimUbicacion\n",
        "print(\"DimUbicacion:\")\n",
        "df_dim_ubicacion.printSchema()\n",
        "df_dim_ubicacion.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GeFZW5BCn4At",
        "outputId": "976c1414-a77b-40fa-8838-b5956abef3eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimUbicacion:\n",
            "root\n",
            " |-- idUbicacion: integer (nullable = false)\n",
            " |-- region: string (nullable = false)\n",
            " |-- country: string (nullable = false)\n",
            " |-- nombre_lugar: string (nullable = false)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- lon: double (nullable = true)\n",
            "\n",
            "+-----------+--------+-------+-------------+------------------+------+\n",
            "|idUbicacion|  region|country| nombre_lugar|               lat|   lon|\n",
            "+-----------+--------+-------+-------------+------------------+------+\n",
            "|          1|Amazonas|   Peru|Amazonas/Peru|-4.999999999682269| -78.0|\n",
            "|          2|  Ancash|   Peru|  Ancash/Peru|-9.499999999871093|-77.75|\n",
            "+-----------+--------+-------+-------------+------------------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Definir el esquema para la DimClima\n",
        "schema_dim_clima = StructType([\n",
        "    StructField(\"idClima\", IntegerType(), False),  # Clave primaria\n",
        "    StructField(\"summary\", StringType(), True),\n",
        "\n",
        "])\n",
        "\n",
        "# Obtener datos de clima únicos\n",
        "data_dim_clima = consolidated_df.select(\"summary\")\n",
        "\n",
        "# Asignar identificadores únicos a través de row_number y Window\n",
        "window = Window.orderBy(\"summary\")\n",
        "data_dim_clima = data_dim_clima.withColumn(\"idClima\", row_number().over(window))\n",
        "\n",
        "# Crear DataFrame para la DimClima\n",
        "df_dim_clima = data_dim_clima.select(\"idClima\", \"summary\")\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame DimClima\n",
        "print(\"DimClima:\")\n",
        "df_dim_clima.printSchema()\n",
        "df_dim_clima.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_zH8MU1vd7r",
        "outputId": "fc5634e7-47c4-4a4a-c37d-c0c29c4b76ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimClima:\n",
            "root\n",
            " |-- idClima: integer (nullable = false)\n",
            " |-- summary: string (nullable = true)\n",
            "\n",
            "+-------+--------------------+\n",
            "|idClima|             summary|\n",
            "+-------+--------------------+\n",
            "|      1|Expect a day of p...|\n",
            "|      2|Expect a day of p...|\n",
            "|      3|Expect a day of p...|\n",
            "|      4|Expect a day of p...|\n",
            "|      5|Expect a day of p...|\n",
            "|      6|Expect a day of p...|\n",
            "|      7|Expect a day of p...|\n",
            "|      8|Expect a day of p...|\n",
            "|      9|Expect a day of p...|\n",
            "|     10|Expect a day of p...|\n",
            "|     11|Expect a day of p...|\n",
            "|     12|The day will star...|\n",
            "|     13|There will be rai...|\n",
            "|     14|There will be rai...|\n",
            "|     15|You can expect pa...|\n",
            "|     16|You can expect ra...|\n",
            "+-------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, col\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType\n",
        "\n",
        "# Renombrar las columnas\n",
        "consolidated_df = consolidated_df.withColumnRenamed(\"temp.day\", \"temp_day\")\\\n",
        "                                 .withColumnRenamed(\"temp.min\", \"temp_min\")\\\n",
        "                                 .withColumnRenamed(\"temp.max\", \"temp_max\")\\\n",
        "                                 .withColumnRenamed(\"temp.night\", \"temp_night\")\\\n",
        "                                 .withColumnRenamed(\"temp.eve\", \"temp_eve\")\\\n",
        "                                 .withColumnRenamed(\"temp.morn\", \"temp_morn\")\n",
        "\n",
        "# Definir el esquema para la DimTemperatura\n",
        "schema_dim_Temperatura = StructType([\n",
        "    StructField(\"idTemperatura\", IntegerType(), False),  # Clave primaria\n",
        "    StructField(\"temp_day\", DoubleType(), True),\n",
        "    StructField(\"temp_min\", DoubleType(), True),\n",
        "    StructField(\"temp_max\", DoubleType(), True),\n",
        "    StructField(\"temp_night\", DoubleType(), True),\n",
        "    StructField(\"temp_eve\", DoubleType(), True),\n",
        "    StructField(\"temp_morn\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Obtener datos de temperatura únicos\n",
        "data_dim_Temperatura = consolidated_df.select(\n",
        "    col(\"temp_day\"),\n",
        "    col(\"temp_min\"),\n",
        "    col(\"temp_max\"),\n",
        "    col(\"temp_night\"),\n",
        "    col(\"temp_eve\"),\n",
        "    col(\"temp_morn\")\n",
        ").distinct()\n",
        "\n",
        "# Asignar identificadores únicos a través de row_number y Window\n",
        "window = Window.orderBy(\"temp_day\", \"temp_min\", \"temp_max\", \"temp_night\", \"temp_eve\", \"temp_morn\")\n",
        "data_dim_Temperatura = data_dim_Temperatura.withColumn(\"idTemperatura\", row_number().over(window))\n",
        "\n",
        "# Crear DataFrame para la DimTemperatura\n",
        "df_dim_Temperatura = data_dim_Temperatura.select(\n",
        "    \"idTemperatura\", \"temp_day\", \"temp_min\", \"temp_max\", \"temp_night\", \"temp_eve\", \"temp_morn\"\n",
        ")\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame DimTemperatura\n",
        "print(\"DimTemperatura:\")\n",
        "df_dim_Temperatura.printSchema()\n",
        "df_dim_Temperatura.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsOFyAuDw88C",
        "outputId": "bc5a8435-1ee9-440d-ffda-86158dad164b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimTemperatura:\n",
            "root\n",
            " |-- idTemperatura: integer (nullable = false)\n",
            " |-- temp_day: double (nullable = true)\n",
            " |-- temp_min: double (nullable = true)\n",
            " |-- temp_max: double (nullable = true)\n",
            " |-- temp_night: double (nullable = true)\n",
            " |-- temp_eve: double (nullable = true)\n",
            " |-- temp_morn: double (nullable = true)\n",
            "\n",
            "+-------------+--------+--------+--------+----------+--------+---------+\n",
            "|idTemperatura|temp_day|temp_min|temp_max|temp_night|temp_eve|temp_morn|\n",
            "+-------------+--------+--------+--------+----------+--------+---------+\n",
            "|            1|  284.35|  281.63|  285.13|    281.63|  282.36|   282.55|\n",
            "|            2|  284.74|  281.55|  284.83|    282.29|  282.88|   282.09|\n",
            "|            3|  285.42|  281.53|  285.42|    281.53|  282.56|   281.94|\n",
            "|            4|  285.68|  281.97|  285.73|    282.78|  282.76|   281.97|\n",
            "|            5|  286.22|  282.41|  286.22|    282.45|   283.5|   282.41|\n",
            "|            6|  286.54|  281.93|  286.54|    283.07|  283.44|   282.35|\n",
            "|            7|  287.03|  282.42|  287.03|    283.56|  283.93|   282.42|\n",
            "|            8|  287.59|  281.58|  287.59|    282.56|  282.88|   281.58|\n",
            "|            9|  295.49|  294.37|  297.37|    294.37|  297.37|   294.88|\n",
            "|           10|  297.08|  294.18|  302.92|    294.58|  301.95|   294.18|\n",
            "|           11|  297.31|  294.65|  300.37|    294.99|  296.17|   295.46|\n",
            "|           12|  297.67|   294.4|  300.55|    295.24|  299.85|   294.48|\n",
            "|           13|  302.58|  295.12|  304.06|    295.94|  299.45|   295.15|\n",
            "|           14|   302.7|  293.96|  303.21|    294.79|  295.74|   295.67|\n",
            "|           15|  304.87|  294.59|  304.87|    294.78|  296.24|   295.46|\n",
            "|           16|  306.22|  294.62|  306.22|     295.3|  296.39|   295.83|\n",
            "+-------------+--------+--------+--------+----------+--------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import row_number, col\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, TimestampType\n",
        "\n",
        "# Renombrar las columnas si es necesario\n",
        "# consolidated_df = consolidated_df.withColumnRenamed(...)\n",
        "\n",
        "# Definir el esquema para la DimLuna\n",
        "schema_dim_Luna = StructType([\n",
        "    StructField(\"idLuna\", IntegerType(), False),  # Clave primaria\n",
        "    StructField(\"moonrise\", TimestampType(), True),\n",
        "    StructField(\"moonset\", TimestampType(), True),\n",
        "    StructField(\"moon_phase\", DoubleType(), True),\n",
        "])\n",
        "\n",
        "# Obtener datos de la Luna únicos\n",
        "data_dim_Luna = consolidated_df.select(\n",
        "    col(\"moonrise\"),\n",
        "    col(\"moonset\"),\n",
        "    col(\"moon_phase\")\n",
        ").distinct()\n",
        "\n",
        "# Asignar identificadores únicos a través de row_number y Window\n",
        "window = Window.orderBy(\"moonrise\", \"moonset\", \"moon_phase\")\n",
        "data_dim_Luna = data_dim_Luna.withColumn(\"idLuna\", row_number().over(window))\n",
        "\n",
        "# Crear DataFrame para la DimLuna\n",
        "df_dim_Luna = data_dim_Luna.select(\n",
        "    \"idLuna\", \"moonrise\", \"moonset\", \"moon_phase\"\n",
        ")\n",
        "\n",
        "# Mostrar el esquema y los primeros registros del DataFrame DimLuna\n",
        "print(\"DimLuna:\")\n",
        "df_dim_Luna.printSchema()\n",
        "df_dim_Luna.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGR_oVfY3XOg",
        "outputId": "177521f6-3fc1-47c8-f707-406a52724dc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DimLuna:\n",
            "root\n",
            " |-- idLuna: integer (nullable = false)\n",
            " |-- moonrise: long (nullable = true)\n",
            " |-- moonset: long (nullable = true)\n",
            " |-- moon_phase: double (nullable = true)\n",
            "\n",
            "+------+----------+----------+----------+\n",
            "|idLuna|  moonrise|   moonset|moon_phase|\n",
            "+------+----------+----------+----------+\n",
            "|     1|1700859240|1700814060|      0.41|\n",
            "|     2|1700859420|1700813820|      0.41|\n",
            "|     3|1700948820|1700903280|      0.44|\n",
            "|     4|1700949120|1700902920|      0.44|\n",
            "|     5|1701038580|1700992680|      0.48|\n",
            "|     6|1701039000|1700992200|      0.48|\n",
            "|     7|1701128340|1701082260|       0.5|\n",
            "|     8|1701128880|1701081660|       0.5|\n",
            "|     9|1701218280|1701171960|      0.55|\n",
            "|    10|1701218820|1701171300|      0.55|\n",
            "|    11|1701307980|1701261720|      0.58|\n",
            "|    12|1701308580|1701261060|      0.58|\n",
            "|    13|1701397620|1701351480|      0.61|\n",
            "|    14|1701398160|1701350820|      0.61|\n",
            "|    15|1701486960|1701441120|      0.64|\n",
            "|    16|1701487380|1701440520|      0.64|\n",
            "+------+----------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Unir df_dim_tiempo con consolidated_df y eliminar duplicados basados en columnas que no son de tipo MAP\n",
        "fact_table_temp = consolidated_df.join(df_dim_tiempo, \"dt\", 'inner').select(consolidated_df[\"*\"], df_dim_tiempo[\"idTiempo\"]).dropDuplicates([c for c in consolidated_df.columns if c != 'weather'])\n",
        "\n",
        "# Unir df_dim_ubicacion con fact_table_temp y eliminar duplicados\n",
        "fact_table = fact_table_temp.join(df_dim_ubicacion, [\"region\", \"country\"], 'inner').select(fact_table_temp[\"*\"], df_dim_ubicacion[\"idUbicacion\"]).dropDuplicates([c for c in consolidated_df.columns if c != 'weather'])\n",
        "\n",
        "# Unir df_dim_clima con fact_table_temp\n",
        "fact_table = fact_table.join(df_dim_clima, \"summary\", 'inner').select(fact_table[\"*\"], df_dim_clima[\"idClima\"]).dropDuplicates([c for c in consolidated_df.columns if c != 'weather'])\n",
        "\n",
        "# Unir df_dim_Temperatura con fact_table utilizando las columnas de temperatura\n",
        "fact_table = fact_table.join(df_dim_Temperatura, [\"temp_day\", \"temp_min\", \"temp_max\", \"temp_night\", \"temp_eve\", \"temp_morn\"], 'inner').select(fact_table[\"*\"], df_dim_Temperatura[\"idTemperatura\"])\n",
        "\n",
        "# Unir df_dim_Luna con fact_table utilizando las columnas relacionadas con la Luna\n",
        "fact_table = fact_table.join(df_dim_Luna, [\"moonrise\", \"moonset\", \"moon_phase\"], 'inner').select(fact_table[\"*\"], df_dim_Luna[\"idLuna\"])\n",
        "\n",
        "\n",
        "# Seleccionar solo las columnas de ID para crear la tabla de hechos\n",
        "fact_table = fact_table.select('idTiempo', 'idUbicacion', 'idTemperatura', 'idLuna','idClima' )\n",
        "# Agregar una columna 'id' a fact_table\n",
        "fact_table = fact_table.withColumn('id', monotonically_increasing_id())\n",
        "\n",
        "# Mover la columna 'id' al principio del DataFrame\n",
        "columns = ['id'] + [col for col in fact_table.columns if col != 'id']\n",
        "fact_table = fact_table.select(columns)\n",
        "\n",
        "print(\"FactTable:\")\n",
        "fact_table.printSchema()\n",
        "fact_table.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiprdSLl3qwa",
        "outputId": "0db1cbb9-52e1-4824-870c-c53919d99099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FactTable:\n",
            "root\n",
            " |-- id: long (nullable = false)\n",
            " |-- idTiempo: long (nullable = false)\n",
            " |-- idUbicacion: integer (nullable = false)\n",
            " |-- idTemperatura: integer (nullable = false)\n",
            " |-- idLuna: integer (nullable = false)\n",
            " |-- idClima: integer (nullable = false)\n",
            "\n",
            "+---+--------+-----------+-------------+------+-------+\n",
            "| id|idTiempo|idUbicacion|idTemperatura|idLuna|idClima|\n",
            "+---+--------+-----------+-------------+------+-------+\n",
            "|  0|       3|          1|           10|     3|     11|\n",
            "|  1|       5|          1|           12|     5|     14|\n",
            "|  2|       7|          1|            9|     7|     16|\n",
            "|  3|       1|          1|           13|     1|     11|\n",
            "|  4|      11|          1|           16|    11|     15|\n",
            "|  5|       9|          1|           15|     9|     11|\n",
            "|  6|      15|          1|           11|    15|     11|\n",
            "|  7|      13|          1|           14|    13|     11|\n",
            "|  8|       3|          2|            8|     4|     11|\n",
            "|  9|       5|          2|            3|     6|     12|\n",
            "| 10|       7|          2|            2|     8|     11|\n",
            "| 11|       1|          2|            4|     2|     11|\n",
            "| 12|      12|          2|            5|    14|     11|\n",
            "| 13|      10|          2|            7|    12|     11|\n",
            "| 14|       8|          2|            6|    10|     14|\n",
            "| 15|      15|          2|            1|    16|     11|\n",
            "+---+--------+-----------+-------------+------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openweatherHive = SparkSession.builder.appName(\"DIMENSIONES-HIVE\").enableHiveSupport().getOrCreate()"
      ],
      "metadata": {
        "id": "sZ-wS8ZMx6ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openweatherHive.sql(\"SHOW DATABASES\").show()\n",
        "openweatherHive.sql(\"CREATE DATABASE IF NOT EXISTS dimensiones\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ufo1-NAuyLQG",
        "outputId": "e2ea0240-0af8-4cc9-f4f6-8d13625c4b30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_dim_tiempo.write.saveAsTable(\"dimensiones.df_dim_tiempo\")\n",
        "df_dim_ubicacion.write.saveAsTable(\"dimensiones.df_dim_ubicacion\")\n",
        "df_dim_clima.write.saveAsTable(\"dimensiones.df_dim_clima\")\n",
        "df_dim_Temperatura.write.saveAsTable(\"dimensiones.df_dim_Temperatura\")\n",
        "df_dim_Luna.write.saveAsTable(\"dimensiones.df_dim_Luna\")"
      ],
      "metadata": {
        "id": "H_8q8Q-OybIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_dim_tiempo.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWdKzQClzYn-",
        "outputId": "e7f51d33-82de-4727-af11-8328612f1f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------------------+----------+----------+-------------+\n",
            "|idTiempo|                 dt|   sunrise|    sunset| nombre_lugar|\n",
            "+--------+-------------------+----------+----------+-------------+\n",
            "|       0|2023-11-24 16:00:00|1700822865|1700867393|Amazonas/Peru|\n",
            "|       1|2023-11-24 16:00:00|1700822391|1700867748|  Ancash/Peru|\n",
            "|       2|2023-11-25 16:00:00|1700909278|1700953817|Amazonas/Peru|\n",
            "|       3|2023-11-25 16:00:00|1700908799|1700954175|  Ancash/Peru|\n",
            "|       4|2023-11-26 16:00:00|1700995692|1701040240|Amazonas/Peru|\n",
            "|       5|2023-11-26 16:00:00|1700995209|1701040603|  Ancash/Peru|\n",
            "|       6|2023-11-27 16:00:00|1701082106|1701126664|Amazonas/Peru|\n",
            "|       7|2023-11-27 16:00:00|1701081619|1701127031|  Ancash/Peru|\n",
            "|       8|2023-11-28 16:00:00|1701168031|1701213460|  Ancash/Peru|\n",
            "|       9|2023-11-28 17:00:00|1701168522|1701213089|Amazonas/Peru|\n",
            "|      10|2023-11-29 16:00:00|1701254443|1701299889|  Ancash/Peru|\n",
            "|      11|2023-11-29 17:00:00|1701254938|1701299514|Amazonas/Peru|\n",
            "|      12|2023-11-30 16:00:00|1701340857|1701386318|  Ancash/Peru|\n",
            "|      13|2023-11-30 17:00:00|1701341355|1701385940|Amazonas/Peru|\n",
            "|      14|2023-12-01 17:00:00|1701427773|1701472366|Amazonas/Peru|\n",
            "|      15|2023-12-01 17:00:00|1701427271|1701472748|  Ancash/Peru|\n",
            "+--------+-------------------+----------+----------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Si se usa colab, ejecutar esta celda\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VkugTnuAH-_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df_dim_tiempo.write.csv('/cont ent/drive/MyDrive/BigData/Tab las/df_dim_tiempo.csv',header=True,mode='overwrite')\n",
        "df_dim_ubicacion.write.csv('/content/drive/MyDrive/BigData/Tablas/df_dim_ubicacion.csv', header=True, mode='overwrite')\n",
        "df_dim_clima.write.csv('/content/drive/MyDrive/BigData/Tablas/df_dim_clima.csv', header=True, mode='overwrite')\n",
        "df_dim_Temperatura.write.csv('/content/drive/MyDrive/BigData/Tablas/df_dim_Temperatura.csv', header=True, mode='overwrite')\n",
        "df_dim_Luna.write.csv('/content/drive/MyDrive/BigData/Tablas/df_dim_Luna.csv', header=True, mode='overwrite')\n",
        "fact_table.write.csv('/content/drive/MyDrive/BigData/Tablas/fact_table.csv', header=True, mode='overwrite')\n"
      ],
      "metadata": {
        "id": "YR9Sw2Q7IByy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GEa2Lu10QLkf"
      }
    }
  ]
}